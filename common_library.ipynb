{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d624430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "# Import necessary libraries for Spark and Delta Lake\n",
    "from delta import DeltaTable\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# Initialize Spark session\n",
    "# This is necessary to use Spark DataFrame and SQL functionalities\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Set Spark configurations for Delta Lake and Parquet\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    concat,\n",
    "    concat_ws,\n",
    "    current_date,\n",
    "    date_format,\n",
    "    dayofmonth,\n",
    "    explode,\n",
    "    from_json,\n",
    "    hour,\n",
    "    lag,\n",
    "    lit,\n",
    "    month,\n",
    "    round,\n",
    "    struct,\n",
    "    to_csv,\n",
    "    to_json,\n",
    "    trim,\n",
    "    udf,\n",
    "    when,\n",
    "    year,\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType,\n",
    "    DateType,\n",
    "    DecimalType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "\n",
    "spark.conf.set('spark.sql.caseSensitive', True)\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbb2b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tip_cast(tipo, precision=None, scale=None):\n",
    "    \"\"\"\n",
    "    Devuelve el tipo de dato de PySpark para castear columnas según el parámetro 'tipo'.\n",
    "    - tipo: 'decimal', 'integer', 'string', 'float', 'double', 'date', 'timestamp', etc.\n",
    "    - precision, scale: solo para decimal.\n",
    "    \"\"\"\n",
    "    from pyspark.sql.types import DecimalType, IntegerType, StringType, FloatType, DoubleType, DateType, TimestampType\n",
    "\n",
    "    if tipo == \"decimal\":\n",
    "        if precision is not None and scale is not None:\n",
    "            return DecimalType(precision=precision, scale=scale)\n",
    "        else:\n",
    "            return DecimalType()\n",
    "    elif tipo == \"integer\":\n",
    "        return IntegerType()\n",
    "    elif tipo == \"string\":\n",
    "        return StringType()\n",
    "    elif tipo == \"float\":\n",
    "        return FloatType()\n",
    "    elif tipo == \"double\":\n",
    "        return DoubleType()\n",
    "    elif tipo == \"date\":\n",
    "        return DateType()\n",
    "    elif tipo == \"timestamp\":\n",
    "        return TimestampType()\n",
    "    else:\n",
    "        raise ValueError(f\"Tipo '{tipo}' no soportado.\")\n",
    "\n",
    "## Ejemplo de uso:\n",
    "# tipo_decimal = get_tip_cast(\"decimal\", 10, 2)\n",
    "# tipo_entero = get_tip_cast(\"integer\")\n",
    "# tipo_texto = get_tip_cast(\"string\")\n",
    "# df = df.withColumn(\"columna_decimal\", F.col(\"columna_decimal\").cast(tipo_decimal))\n",
    "# df = df.withColumn(\"columna_entero\", F.col(\"columna_entero\").cast(tipo_entero))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d44412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(dforigen, selected_cols, new_col_names):\n",
    "    df_filtered = dforigen.select(selected_cols)\n",
    "    df_renamed = df_filtered.toDF(*new_col_names)\n",
    "    return df_renamed\n",
    "\n",
    "\n",
    "## Ejemplo de uso:\n",
    "## Supón que tienes un DataFrame con columnas: [\"A\", \"B\", \"C\"]\n",
    "#df_renamed = rename_columns(df, [\"A\", \"B\"], [\"Columna1\", \"Columna2\"])\n",
    "## Ahora df_renamed tendrá solo dos columnas: [\"Columna1\", \"Columna2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_simple_transform_data(origin_table, pi_fecha_carga_datos, pi_proceso_carga_datos, important_fields, renamed_fields, columns_to_transform_to_integer, columns_to_transform_to_float, columns_to_transform_to_double, columns_to_transform_to_decimal, columns_to_transform_to_date, columns_to_transform_to_timestamp, columns_to_transform_to_bool = [], filtro_opcional=\"\", decimal_columns={}):\n",
    "    df = spark.sql(f\"SELECT {', '.join(important_fields)} FROM {pi_lakehouse_origen}.{origin_table} WHERE FECHA_CARGA_DATOS = '{pi_fecha_carga_datos}'\" + (f\" {filtro_opcional}\" if filtro_opcional else \"\"))\n",
    "    df_renamed = rename_columns(df, important_fields, renamed_fields)\n",
    "    \n",
    "    if decimal_columns:\n",
    "        \n",
    "        list_of_columns_to_transform_to_decimal =[(col(c).cast(DecimalType(precision=38, scale=decimal_columns[c]))) for c in decimal_columns]\n",
    "\n",
    "    else: \n",
    "        list_of_columns_to_transform_to_decimal = [col(c).cast('decimal') for c in columns_to_transform_to_decimal] #Por defecto es con 0 decimales\n",
    "    \n",
    "    df_cast = df_renamed.select(\\\n",
    "            *[col(c).cast('integer') for c in columns_to_transform_to_integer],\\\n",
    "            *[col(c).cast('float') for c in columns_to_transform_to_float],\\\n",
    "            *[col(c).cast('double') for c in columns_to_transform_to_double],\\\n",
    "            *list_of_columns_to_transform_to_decimal,\\\n",
    "            *[col(c).cast('date') for c in columns_to_transform_to_date],\\\n",
    "            *[col(c).cast('timestamp') for c in columns_to_transform_to_timestamp],\\\n",
    "            *[col(c).cast('boolean') for c in columns_to_transform_to_bool],\\\n",
    "            *[col(c) for c in df_renamed.columns if c not in columns_to_transform_to_integer+columns_to_transform_to_float+columns_to_transform_to_double+columns_to_transform_to_decimal+columns_to_transform_to_date+columns_to_transform_to_timestamp+columns_to_transform_to_bool]\\\n",
    "        )\n",
    "    column_order = df_renamed.columns\n",
    "    df_cast = df_cast.select(*column_order)\n",
    "    df_cast = df_cast.select(\"*\", lit(pi_proceso_carga_datos).alias(\"AuditoriaProcesoCargaDatos\"),\\\n",
    "    lit(pi_fecha_carga_datos).alias(\"AuditoriaFechaCargaDatos\"))\n",
    "    return df_cast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc084d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_write_params_overwriteschema(is_dev):\n",
    "    if is_dev:\n",
    "        write_params = True\n",
    "    else: \n",
    "        write_params = False\n",
    "    return write_params\n",
    "\n",
    "## No entiendo el uso de la funcion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c57a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_table_lh(workspace, lakehouse, schema, table):\n",
    "    # Validación simple de parámetros\n",
    "    if any(\" \" in str(x) for x in [workspace, lakehouse, schema, table]):\n",
    "        raise ValueError(\"Ningún parámetro debe contener espacios en blanco.\")\n",
    "    # Construcción de la ruta\n",
    "    table_path = f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/{lakehouse}.Lakehouse/Tables/{schema}/{table}\"\n",
    "    # Lectura de la tabla\n",
    "    df = spark.read.load(table_path)\n",
    "    return df\n",
    "\n",
    "## Ejemplo de uso:\n",
    "# Esta función se utiliza para leer una tabla de un Lakehouse en OneLake.\n",
    "## Supón que tienes los siguientes datos:\n",
    "# workspace = \"miworkspace\"\n",
    "# lakehouse = \"milakehouse\"\n",
    "# schema = \"miesquema\"\n",
    "# table = \"mitabla\"\n",
    "\n",
    "## Llamas a la función para leer la tabla:\n",
    "#df = read_table_lh(workspace, lakehouse, schema, table)\n",
    "\n",
    "## Ahora puedes trabajar con el DataFrame 'df', por ejemplo:\n",
    "#df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a95d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_or_create_schema(workspace, lakehouse, schema_name):\n",
    "    list_of_schemas = []\n",
    "    for schema in notebookutils.fs.ls( f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/{lakehouse}.Lakehouse/Tables/\"):\n",
    "        list_of_schemas.append(schema.name)\n",
    "    if schema_name not in list_of_schemas:\n",
    "        notebookutils.fs.mkdirs(f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/{lakehouse}.Lakehouse/Tables/{schema_name}\")\n",
    "        print(f\"El esquema {schema_name} se ha creado.\")\n",
    "    else:\n",
    "        print(f\"El esquema {schema_name} ya existe en el entorno de trabajo especificado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282143e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mes_nombre = {1: \"Enero\", 2: \"Febrero\", 3: \"Marzo\", 4: \"Abril\",\\\n",
    "    5: \"Mayo\", 6: \"Junio\", 7: \"Julio\", 8: \"Agosto\",\\\n",
    "    9: \"Septiembre\", 10: \"Octubre\", 11: \"Noviembre\", 12: \"Diciembre\"}\n",
    "\n",
    "@udf(StringType())\n",
    "def cambiar_idioma_mes(month):\n",
    "    return mes_nombre.get(month, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2880b9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_segmentos_fecha(columna):\n",
    "   return [year(col(columna)).alias(f\"{columna}_Año\"),\\\n",
    "           month(col(columna)).alias(f\"{columna}_Mes\"),\\\n",
    "           cambiar_idioma_mes(month(col(columna))).alias(f\"{columna}_MesNombre\"),\\\n",
    "           dayofmonth(col(columna)).alias(f\"{columna}_Dia\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096960ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_segmentos_fecha(columna):\n",
    "    \"\"\"\n",
    "    Extrae diferentes segmentos de una columna que representa fechas.\n",
    "\n",
    "    :param columna: La columna que contiene las fechas.\n",
    "    :return: Una lista con nuevas columnas que representan el año, mes y nombre del mes de la fecha.\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import year, month, dayofmonth, col\n",
    "\n",
    "    # Extrae el año\n",
    "    año = year(col(columna)).alias(f\"{columna}_Año\")\n",
    "\n",
    "    # Extrae el mes (numérico)\n",
    "    mes_numérico = month(col(columna)).alias(f\"{columna}_Mes\")\n",
    "\n",
    "    # Convierte el mes numérico a nombre en español\n",
    "    mes_nombre = cambiar_idioma_mes(mes_numérico).alias(f\"{columna}_MesNombre\")\n",
    "\n",
    "    # Extrae el día del mes\n",
    "    dia_del_mes = dayofmonth(col(columna)).alias(f\"{columna}_Dia\")\n",
    "\n",
    "    return [año, mes_nombre, dia_del_mes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec7e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_segmentos_hora(columna):\n",
    "   return [hour(col(columna)).alias(f\"{columna}_Hora\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617cd3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_mapeo(lakehouse_path, tabla, campoclave, campovalor1, campovalor2=None):\n",
    "    \n",
    "    table_path = lakehouse_path+\"/\"+tabla\n",
    "\n",
    "    df = spark.read.load(table_path)\n",
    "    \n",
    "    dictmapeo1 = dict(df.select(campoclave, campovalor1).rdd.map(lambda row: (row[campoclave], row[campovalor1])).collect())\n",
    "    \n",
    "    if campovalor2:\n",
    "        dictmapeo2 = dict(df.select(campoclave, campovalor2).rdd.map(lambda row: (row[campoclave], row[campovalor2])).collect())\n",
    "        return dictmapeo1, dictmapeo2\n",
    "    \n",
    "    return dictmapeo1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2869f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traducir_mapeo_diccionario(dictionary): \n",
    "    return udf(lambda col: dictionary.get(col),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a87e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df_cast, types_dict):\n",
    "    transform_exprs = []\n",
    "    transformed_columns = []\n",
    "    for colname, target_type in types_dict.items(): \n",
    "        if colname in df_cast.columns:\n",
    "            transform_exprs.append(col(colname).cast(target_type))\n",
    "            transformed_columns.append(colname)\n",
    "        else:\n",
    "            print(\"Revisa la columna \"+ colname+ \". No está en el df\")\n",
    "    for column in df_cast.columns:\n",
    "        if not column in transformed_columns:\n",
    "            transform_exprs.append(col(column))\n",
    "    return df_cast.select(*transform_exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a322623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def read_and_transform_data(origin_table, pi_fecha_carga_datos, pi_proceso_carga_datos, important_fields, renamed_fields, filtro_opcional=\"\", types_dict={}):\n",
    "#    df = spark.sql(f\"SELECT {', '.join(important_fields)} FROM {pi_lakehouse_origen}.{origin_table} WHERE FECHA_CARGA_DATOS = '{pi_fecha_carga_datos}'\" + (f\" {filtro_opcional}\" if filtro_opcional else \"\"))\n",
    "#    df_renamed = rename_columns(df, important_fields, renamed_fields)\n",
    "#    \n",
    "#    df_cast = transform_data(df_renamed, types_dict)\n",
    "#\n",
    "#    column_order = df_renamed.columns\n",
    "#    df_cast = df_cast.select(*column_order)\n",
    "#    df_cast = df_cast.select(\"*\", lit(pi_proceso_carga_datos).alias(\"AuditoriaProcesoCargaDatos\"),\\\n",
    "#    lit(pi_fecha_carga_datos).alias(\"AuditoriaFechaCargaDatos\"))\n",
    "#    return df_cast\n",
    "\n",
    "def read_and_transform_data(\n",
    "    origin_table,\n",
    "    pi_fecha_carga_datos,\n",
    "    pi_proceso_carga_datos,\n",
    "    important_fields,\n",
    "    renamed_fields,\n",
    "    filtro_opcional=\"\",\n",
    "    types_dict={},\n",
    "    schema=None,\n",
    "):\n",
    "    tabla_origen = (\n",
    "        f\"{pi_lakehouse_origen}.{schema}.{origin_table}\"\n",
    "        if schema\n",
    "        else f\"{pi_lakehouse_origen}.{origin_table}\"\n",
    "    )\n",
    "    df = spark.sql(\n",
    "        f\"SELECT {', '.join(important_fields)} FROM {tabla_origen} WHERE FECHA_CARGA_DATOS = '{pi_fecha_carga_datos}'\"\n",
    "        + (f\" {filtro_opcional}\" if filtro_opcional else \"\")\n",
    "    )\n",
    "    df_renamed = rename_columns(df, important_fields, renamed_fields)\n",
    "\n",
    "    df_cast = transform_data(df_renamed, types_dict)\n",
    "\n",
    "    column_order = df_renamed.columns\n",
    "    df_cast = df_cast.select(*column_order)\n",
    "    df_cast = df_cast.select(\n",
    "        \"*\",\n",
    "        lit(pi_proceso_carga_datos).alias(\"AuditoriaProcesoCargaDatos\"),\n",
    "        lit(pi_fecha_carga_datos).alias(\"AuditoriaFechaCargaDatos\"),\n",
    "    )\n",
    "    return df_cast    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027bfec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_table_into_deltalake(\n",
    "    df,\n",
    "    workspace,\n",
    "    pi_lakehouse_destino,\n",
    "    schema,\n",
    "    tabla_destino,\n",
    "    primary_keys,\n",
    "    modo=\"upsert\",\n",
    "    ow_schema=False,\n",
    "    del_fec_baja=True,\n",
    "    is_lh_name = True\n",
    "):\n",
    "    try:\n",
    "        # Chequeamos que el WS no tenga espacios. Falla en Fabric\n",
    "        if len(str(workspace).split(' ')) > 1:\n",
    "            print('Error: El workspace no puede tener espacios en blanco')\n",
    "            return\n",
    "\n",
    "        # Chequeamos si estamos recibiendo la info del Lakehouse destino como nombre o como id, para añadir la extensión '.Lakehouse' en caso de nombre. Por defecto, asumimos recibir nombres\n",
    "        if is_lh_name:\n",
    "            pi_lakehouse_destino = pi_lakehouse_destino+'.Lakehouse'\n",
    "\n",
    "        # Chequeamos si la PK/s es un string o una lista. Lo pasamos siempre a lista para que no de problemas el upsert (en su caso)\n",
    "        if type(primary_keys) == str:\n",
    "            primary_keys = [primary_keys]\n",
    "        \n",
    "        # Chequeamos si el DF de entrada tiene o no fecha de baja (encuentra cualquiera que contenga 'AuditoriaFechaBaja'. Ej: 'AuditoriaFechaBaja' y 'AuditoriaFechaBajaLogica')\n",
    "        col_fec_baja = ''\n",
    "\n",
    "        for col in df.columns:\n",
    "            if 'AuditoriaFechaBaja' in col:\n",
    "                col_fec_baja = col\n",
    "                break\n",
    "\n",
    "        # Ruta final de guardado. El nombre del Lakehouse es mejor pasarlo como '\"nombre_lakehouse\".Lakehouse', por tema de id-id o nombre-nombre\n",
    "        abfs_path = f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/{pi_lakehouse_destino}/Tables/{schema}/{tabla_destino}\"\n",
    "\n",
    "        list_of_table_names = []\n",
    "        for table in mssparkutils.fs.ls(\n",
    "            f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/{pi_lakehouse_destino}/Tables/{schema}\"\n",
    "        ):\n",
    "            list_of_table_names.append(table.name)\n",
    "\n",
    "        # Casuística de si existe o no la tabla (se comenta y quita la parte de is_dev para hacer una función más genérica -> \"and is_dev\" y el parámetro)\n",
    "        if (not tabla_destino in list_of_table_names):\n",
    "            if col_fec_baja != '':\n",
    "                if del_fec_baja:\n",
    "                    # Si lo hemos seleccionado como parámetro, borramos los registros dados de baja\n",
    "                    df = df.filter(df[col_fec_baja].isNull())\n",
    "                \n",
    "                # Borramos siempre, si existe, la columna de fecha de baja porque no suele estar en la capa std (REVISAR ESTO!)\n",
    "                df = df.drop(col_fec_baja)\n",
    "\n",
    "            # Si la tabla destino no existe en el Lakehouse, el modo siempre es 'overwrite' con 'overwriteSchema' por defecto (independientemente del campo modo)\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"{abfs_path}\")\n",
    "            print(\"Table saved (new)\")\n",
    "        else:\n",
    "            if modo == \"upsert\":\n",
    "                if col_fec_baja != '':\n",
    "                    if del_fec_baja:\n",
    "                        # Si vamos a querer borrar los registros dados de baja, nos guardamos cuáles son\n",
    "                        df_fecha_baja_not_null = df.filter(df[col_fec_baja].isNotNull()).drop(col_fec_baja) #.select(primary_keys)\n",
    "                    \n",
    "                    df = df.drop(col_fec_baja)\n",
    "\n",
    "                # Si tenemos más de una PK en la lista de entrada, hemos de establecer la condición del merge con todas ellas\n",
    "                condition_list = []\n",
    "                for col_pivot in primary_keys:  \n",
    "                    condition_list.append(f'destino.{col_pivot} = origen.{col_pivot}')\n",
    "                condition=' AND '.join(condition_list)\n",
    "\n",
    "                # Realizamos el merge\n",
    "                tabla_delta = DeltaTable.forPath(spark, abfs_path)\n",
    "                tabla_delta.alias(\"destino\").merge(\n",
    "                    df.alias(\"origen\"), condition).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "                if col_fec_baja != '' and del_fec_baja:\n",
    "                    if not df_fecha_baja_not_null.rdd.isEmpty():\n",
    "\n",
    "                        # Borramos de la tabla destino, aquellos registros que se hayan actualizado con FEC_BAJA != NULL\n",
    "                        tabla_delta.alias(\"destino\").merge(\n",
    "                            df_fecha_baja_not_null.alias(\"origen\"),\n",
    "                            condition).whenMatchedDelete().execute()\n",
    "\n",
    "                        # Para mostrar, si queremos, los registros borrados (en su caso)\n",
    "                        primary_key_list = (\n",
    "                            df_fecha_baja_not_null.select(primary_keys).rdd.map(tuple).collect()\n",
    "                        )\n",
    "                        print(\"deleted \" + str(primary_key_list))\n",
    "                print(\"Table saved (upsert)\")\n",
    "            elif modo == \"append\":\n",
    "                # Aquí habría que tener cuidado con la presencia de PKs repetidas, aunque si se hace append es porque se quiere añadir todo el conjunto nuevo\n",
    "                # Si hay registros dados de baja en el bloque a añadir, se quitan y se elimina el campo de fecha de baja (que no suele estar en la capa std)\n",
    "                if col_fec_baja != '':\n",
    "                    if del_fec_baja:\n",
    "                        df = df.filter(df[col_fec_baja].isNull())\n",
    "                    \n",
    "                    df = df.drop(col_fec_baja)\n",
    "\n",
    "                df.write.format(\"delta\").mode(\"append\").save(f\"{abfs_path}\")\n",
    "                print(\"Table saved (append)\")\n",
    "            elif modo == \"overwrite\":\n",
    "                # Si hay registros dados de baja en el bloque con el que vamos a sobreescribir, se quitan si lo deseamos, y se elimina el campo de fecha de baja (que no suele estar en la capa std)\n",
    "                if col_fec_baja != '':\n",
    "                    if del_fec_baja:\n",
    "                        df = df.filter(df[col_fec_baja].isNull())\n",
    "                        \n",
    "                    df = df.drop(col_fec_baja)\n",
    "\n",
    "                # Decidimos en base al parámetro 'ow_schema' si queremos sobreescribir el esquema de la tabla o no (en dev típicamente sí, pero en pro no)\n",
    "                if ow_schema:\n",
    "                    df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"{abfs_path}\")\n",
    "                else:\n",
    "                    df.write.format(\"delta\").mode(\"overwrite\").save(f\"{abfs_path}\")\n",
    "                print(\"Table saved (overwrite)\")\n",
    "            else:\n",
    "                print(f\"Error: Mode {modo} not valid. Allowed values: overwrite, append or upsert\")\n",
    "    except:\n",
    "        print(\"Error en la función de escritura. Escritura no realizada\")\n",
    "        df.show(n=5, vertical=True, truncate=False)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151894ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_url(url, headers=None, params=None):\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "    if response.ok:\n",
    "        try:\n",
    "            response = response.json()\n",
    "        except Exception as e:\n",
    "            print(f\"Decoding error: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        msg = f\"status {str(response.status_code)}, details: {response.text}\"\n",
    "        print(f\"API call error: {msg}\")\n",
    "        return None\n",
    "\n",
    "    return response\n",
    "\n",
    "## Ejemplo de uso:\n",
    "# import requests\n",
    "\n",
    "## URL de la API que deseas consultar\n",
    "#url = \"https://api.example.com/data\"\n",
    "\n",
    "## Parámetros adicionales si es necesario\n",
    "#params = {\n",
    "#  \"param1\": \"value1\",\n",
    "#  \"param2\": \"value2\"\n",
    "#}\n",
    "\n",
    "## Encabezados adicionales si es necesario\n",
    "#headers = {\n",
    "#   \"Authorization\": \"Bearer your_access_token_here\"\n",
    "#}\n",
    "\n",
    "## Llamada a la función\n",
    "#data = get_data_from_url(url, headers=headers, params=params)\n",
    "\n",
    "#if data is not None:\n",
    "#   print(\"Datos obtenidos exitosamente:\")\n",
    "#  print(data)\n",
    "#else:\n",
    "#    print(\"Error al obtener los datos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6e1b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_url(url, headers=None, params=None):\n",
    "    \"\"\"\n",
    "    Obtiene datos de una URL utilizando una solicitud GET a través de la biblioteca requests.\n",
    "    \n",
    "    :param url: URI de la API al que se realizará la solicitud.\n",
    "    :param headers: Diccionario de encabezados adicionales (opcional).\n",
    "    :param params: Diccionario de parámetros adicionales para la solicitud GET (opcional).\n",
    "    :return: Los datos decodificados del JSON si la respuesta es exitosa, None en caso contrario.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        \n",
    "        if response.ok:\n",
    "            # Añadir excepción más específica para errores de codificación JSON\n",
    "            return response.json()\n",
    "        else:\n",
    "            msg = f\"status {response.status_code}, details: {response.text}\"\n",
    "            raise Exception(f\"API call error: {msg}\")\n",
    "    \n",
    "    except requests.exceptions.JSONDecodeError as e:\n",
    "        print(f\"Decoding error: {e}\")\n",
    "        return None\n",
    "\n",
    "## Ejemplo de uso\n",
    "#import logging\n",
    "\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "#url = \"https://api.example.com/data\"\n",
    "#params = {\n",
    "#    \"param1\": \"value1\",\n",
    "#    \"param2\": \"value2\"\n",
    "#}\n",
    "#headers = {\n",
    "#    \"Authorization\": \"Bearer your_access_token_here\"\n",
    "#}\n",
    "\n",
    "#try:\n",
    "#    data = get_data_from_url(url, headers=headers, params=params)\n",
    "#    if data is not None:\n",
    "#        logging.info(\"Datos obtenidos exitosamente:\")\n",
    "#        logging.info(data)\n",
    "#    else:\n",
    "#        logging.error(\"Error al obtener los datos.\")\n",
    "#except Exception as e:\n",
    "#    logging.error(f\"Ocurrió un error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec9dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_json_col(df, col_name, schema, list_selected_cols):\n",
    "    '''\n",
    "    df: DataFrame name we want to modify\n",
    "    col_name: Column name we want to normalize\n",
    "    shcema: JSON schema in PySpark types \n",
    "    selected_cols: Column names we want to select\n",
    "    '''\n",
    "    normalized_df = df.withColumn(col_name, from_json(col(col_name), schema))\n",
    "    normalized_df = normalized_df.withColumn(col_name, explode(col_name))\n",
    "    normalized_df = normalized_df.select([col(c) for c in list_selected_cols])\n",
    "    \n",
    "    return normalized_df\n",
    "\n",
    "## Ejemplo de uso:\n",
    "\n",
    "#from pyspark.sql import SparkSession\n",
    "\n",
    "## Inicializar una sesión de Spark\n",
    "#spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "\n",
    "## Ejemplo de DataFrame con columnas JSON\n",
    "#data = [\n",
    "#   (1, '{\"name\": \"Alice\", \"age\": 30, \"address\": {\"city\": \"New York\"}}'),\n",
    "#   (2, '{\"name\": \"Bob\", \"age\": 25, \"address\": {\"city\": \"Los Angeles\"}}')\n",
    "#]\n",
    "#columns = [\"id\", \"json_data\"]\n",
    "\n",
    "#df = spark.createDataFrame(data, columns)\n",
    "\n",
    "## Definir el esquema JSON\n",
    "#schema = {\n",
    "#    'id': IntegerType(),\n",
    "#    'name': StringType(),\n",
    "#    'age': IntegerType(),\n",
    "#    'address': StructType([\n",
    "#     ('city', StringType())\n",
    "#    ])\n",
    "#}\n",
    "\n",
    "# Columna a normalizar y las columnas seleccionadas\n",
    "#col_to_normalize = \"json_data\"\n",
    "#selected_columns = [\"id\", \"name\", \"city\"]\n",
    "\n",
    "# Llamar a la función normalize_json_col\n",
    "#normalized_df = normalize_json_col(df, col_to_normalize, schema, selected_columns)\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "#normalized_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14fd01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_json_col(df, col_name, schema):\n",
    "    '''\n",
    "    df: DataFrame name we want to modify\n",
    "    col_name: Column name we want to normalize\n",
    "    shcema: JSON schema in PySpark types \n",
    "    '''\n",
    "    # Convert the JSON column into a structured format using from_json\n",
    "    normalized_df = df.withColumn(col_name, from_json(col(col_name), schema))\n",
    "    \n",
    "    return normalized_df\n",
    "\n",
    "## Ejemplo de uso:\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "\n",
    "#    data = [\n",
    "#        (1, '{\"name\": \"Alice\", \"age\": 30, \"address\": {\"city\": \"New York\"}}'),\n",
    "#        (2, '{\"name\": \"Bob\", \"age\": 25, \"address\": {\"city\": \"Los Angeles\"}}')\n",
    "#    ]\n",
    "#    columns = [\"id\", \"json_data\"]\n",
    "\n",
    "#    df = spark.createDataFrame(data, columns)\n",
    "\n",
    "    # Define the JSON schema\n",
    "#    schema = {\n",
    "#        'id': IntegerType(),\n",
    "#        'name': StringType(),\n",
    "#        'age': IntegerType(),\n",
    "#        'address': StructType([\n",
    "#            ('city', StringType())\n",
    "#        ])\n",
    "#    }\n",
    "\n",
    "#3    col_to_normalize = \"json_data\"\n",
    "\n",
    "## Normalize the column and select specific columns\n",
    "#    normalized_df = normalize_json_col(df, col_to_normalize, schema)\n",
    "\n",
    "## Use json_normalize to simplify the data extraction\n",
    "#    normalized_df_normalized = json_normalize(normalized_df.select(col_name))\n",
    "\n",
    "## Show the result\n",
    "# normalized_df_normalized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f76830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jerarquizar_fechas(df, lista_columnas):\n",
    "    if all(col in df.columns for col in lista_columnas):\n",
    "        # Se extrae fecha (año, mes y día)\n",
    "        columnas_jerarquia_fechas = []\n",
    " \n",
    "        for column in lista_columnas:\n",
    "            columnas_jerarquia_fechas.extend(extraer_segmentos_fecha(column))\n",
    "        df = df.select(*df.columns, *columnas_jerarquia_fechas)\n",
    "    else:\n",
    "        print(\"No todas las columnas estaban en el DataFrame, proceso no ejecutado.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1bc09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jerarquizar_fechas(df, lista_columnas):\n",
    "    # Verificar si todas las columnas están presentes en el DataFrame\n",
    "    if not all(col in df.columns for col in lista_columnas):\n",
    "        print(\"No todas las columnas estaban en el DataFrame, proceso no ejecutado.\")\n",
    "        return df\n",
    "\n",
    "    # Extraer segmentos de fecha (año, mes y día) para todas las columnas especificadas\n",
    "    fechas_jerarquizadas = []\n",
    "    for col in lista_columnas:\n",
    "        fecha_segmentos = extraer_segmentos_fecha(col)\n",
    "        if fecha_segmentos:\n",
    "            fechas_jerarquiaizadas.extend(fecha_segmentos)\n",
    "\n",
    "    # Seleccionar el DataFrame para incluir todas las columnas actuales y los nuevos segmentos de fecha\n",
    "    df = df.select(*df.columns, *fechas_jerarquizadas)\n",
    "\n",
    "    return df\n",
    "\n",
    "##Ejemplo de uso:\n",
    "#import pandas as pd\n",
    "\n",
    "## Creación del DataFrame\n",
    "#data = {\n",
    "#    'fecha1': ['2023-04-15', '2023-05-20', '2023-06-25'],\n",
    "#    'fecha2': ['2023-12-25', '2024-01-01', '2024-02-05']\n",
    "#}\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "## Lista de columnas a jerarquizar\n",
    "#columnas_a_jerarquizar = ['fecha1', 'fecha2']\n",
    "\n",
    "## Llamada a la función jerarquizar_fechas\n",
    "#resultado = jerarquizar_fechas(df, columnas_a_jerarquizar)\n",
    "\n",
    "#print(resultado)\n",
    "\n",
    "# fecha1   fecha2  año_f1  mes_f1  día_f1  año_f2  mes_f2  día_f2\n",
    "#0  2023-04-15  2023-12-25     2023      4       15        2023    12       25\n",
    "#1  2023-05-20  2024-01-01     2023      5       20        2024      1       01\n",
    "#2  2023-06-25  2024-02-05     2023      6       25        2024      2       05\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da652a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jerarquizar_horas(df, lista_columnas):\n",
    "    if all(col in df.columns for col in lista_columnas):\n",
    "        # Se extrae tiempo (hora, minutos y segundos)\n",
    "        columnas_jerarquia_fechas = []\n",
    " \n",
    "        for column in lista_columnas:\n",
    "            columnas_jerarquia_fechas.extend(extraer_segmentos_hora(column))\n",
    "        df = df.select(*df.columns, *columnas_jerarquia_fechas)\n",
    "    else:\n",
    "        print(\"No todas las columnas estaban en el DataFrame, proceso no ejecutado.\")\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
