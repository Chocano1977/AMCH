{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e381bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las librerías necesarias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DataType\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ce92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer los parámetros\n",
    "# Pongo valores por defecto para cuando ejecute el notebook individualmente, pero al marcar esta \"celda\" como de Parámetros, se sobrescriben desde Pipeline\n",
    "fecha_proceso = \"1900-01-01\" \n",
    "nombre_fichero = \"nombre_fichero_por_defecto.csv\" \n",
    "\n",
    "# Mostrarlos\n",
    "print(f\"Fecha de proceso: {fecha_proceso}\")\n",
    "print(f\"Nombre de fichero: {nombre_fichero}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7837be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paso 1: Cargar el fichero desde el entorno de Fabric (OneLake)\n",
    "#file_path = \"Files/origen1/CAMPOS-INFR-POR-PROYECTO.csv\"\n",
    "file_path = \"Files/origen1/\"+nombre_fichero\n",
    "\n",
    "# Paso 2: Leer el fichero como DataFrame\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"delimiter\", \";\").csv(file_path)\n",
    "\n",
    "\n",
    "# Paso 2: Añadir columna de fecha y hora actual\n",
    "df_con_fecha = df.withColumn(\"fecha_carga\", current_timestamp())\n",
    "\n",
    "# Mostrar los primeros registros para verificar\n",
    "df_con_fecha.show()\n",
    "\n",
    "# Paso 3: Crear la tabla en el Lakehouse\n",
    "myWorkspace = \"ws_tte_poc_data\"\n",
    "myLakehouse = \"raw_lh_app1_origen1\"\n",
    "mytable = \"tabla1\"\n",
    "mySchema = \"staging\"\n",
    "tablePath= mySchema+\".\"+mytable\n",
    "\n",
    "## En STAGING OVERWRITE::\n",
    "df_con_fecha.write.mode(\"overwrite\").saveAsTable(tablePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6beaef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4: Insertar log\n",
    "#creo la fila para el log\n",
    "\n",
    "strLOG = \"Param_fichero:\"+nombre_fichero+\" Param fecha: \"+fecha_proceso+\"Tabla TABLA1 cargada en esquema STAGING\"\n",
    "fecha_actual = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"STAGING LOADED \"+fecha_actual)\n",
    "row1 = Row(str_workspace=myWorkspace, str_lakehouse=myLakehouse, str_schema=mySchema, str_table=mytable, STR_MESSAGE=strLOG, STR_LOAD_DATE=fecha_actual)\n",
    "\n",
    "\n",
    "df_log = spark.createDataFrame([row1])\n",
    "\n",
    "print(\"el df de log contiene:\")\n",
    "print (df_log.toJSON)\n",
    "\n",
    "# Insertar el registro en la tabla del Lakehouse\n",
    "logLakehouse = \"ctrl_lh_app1\"\n",
    "logtable=\"loadlogs\"\n",
    "logSchema = \"log\"\n",
    "logtablePath= logLakehouse+\".\"+logSchema+\".\"+logtable\n",
    "df_log.write.mode(\"append\").saveAsTable(logtablePath)\n",
    "\n",
    "print(\"insertado log\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
