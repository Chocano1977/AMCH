{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a564508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f89af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_std_config_files(\n",
    "    file_name, \n",
    "    merge_fields, \n",
    "    schema_name=\"config\", \n",
    "    lakehouse_path=\"abfss://ws_tte_poc_data@onelake.dfs.fabric.microsoft.com/std_lh_tte_dominioEnagas.Lakehouse\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Procesa un archivo CSV dentro del mismo Lakehouse, lo guarda como tabla Delta y realiza un MERGE con una tabla existente.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): Nombre del archivo CSV (sin ruta completa).\n",
    "        merge_fields (list): Lista de campos con el mismo nombre en origen y destino para realizar el MERGE.\n",
    "        schema_name (str): Esquema donde se guardará la tabla Delta. Por defecto, \"config\".\n",
    "        lakehouse_path (str): Ruta base del Lakehouse. Por defecto, una ruta predefinida.\n",
    "    \"\"\"\n",
    "    # Construir la ruta completa al archivo CSV\n",
    "    csv_file_path = f\"{lakehouse_path}/Files/config_schema/{file_name}\"\n",
    "\n",
    "    # Extraer el nombre del archivo sin extensión para usarlo como nombre de tabla\n",
    "    table_name = f\"{schema_name}.{Path(file_name).stem}\"\n",
    "    delta_path = f\"{lakehouse_path}/Tables/{schema_name}/{Path(file_name).stem}\"\n",
    "\n",
    "    # Leer archivo CSV desde ABFSS con separador ';'\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"sep\", \";\") \\\n",
    "        .load(csv_file_path)\n",
    "\n",
    "    # Mostrar los datos leídos\n",
    "    display(df)\n",
    "\n",
    "    # Crear tabla Delta solo si no existe en el esquema\n",
    "    if not spark.catalog.tableExists(table_name):\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "        spark.sql(f\"CREATE TABLE {table_name} USING DELTA LOCATION '{delta_path}'\")\n",
    "\n",
    "    # Cargar la tabla Delta existente\n",
    "    from delta.tables import DeltaTable\n",
    "    delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "    # Crear una vista temporal con los nuevos datos\n",
    "    df.createOrReplaceTempView(\"updates\")\n",
    "\n",
    "    # Generar las condiciones de MERGE dinámicamente\n",
    "    merge_conditions = \" AND \".join([f\"target.{field} = source.{field}\" for field in merge_fields])\n",
    "\n",
    "    # Realizar el MERGE (upsert)\n",
    "    delta_table.alias(\"target\").merge(\n",
    "        df.alias(\"source\"),\n",
    "        merge_conditions\n",
    "    ).whenMatchedUpdateAll() \\\n",
    "     .whenNotMatchedInsertAll() \\\n",
    "     .execute()\n",
    "\n",
    "    # Consultar los datos de la tabla para ver los resultados\n",
    "    display(spark.sql(f\"SELECT * FROM {table_name}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9894dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga fichero column_mapping.csv\n",
    "file_name = \"column_mapping.csv\"\n",
    "merge_fields = [\"schema_destino\", \"tabla_origen\", \"columna_origen\"]\n",
    "\n",
    "load_std_config_files(file_name, merge_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625d029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga fichero validation_config\n",
    "file_name = \"validation_config.csv\"\n",
    "merge_fields = [\"tabla_origen\", \"pk_columna\",\"regla_tipo\",\"columna_1\"]\n",
    "\n",
    "load_std_config_files(file_name, merge_fields)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
