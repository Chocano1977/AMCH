{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d624430b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spark\n\u001b[1;32m---> 14\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mnew_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Set Spark configurations for Delta Lake and Parquet\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     col,\n\u001b[0;32m     19\u001b[0m     concat,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     year,\n\u001b[0;32m     38\u001b[0m )\n",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m, in \u001b[0;36mnew_func\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnew_func\u001b[39m():\n\u001b[1;32m---> 11\u001b[0m     spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spark\n",
      "File \u001b[1;32mc:\\Users\\EN31380\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    554\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    555\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\EN31380\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\core\\context.py:523\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 523\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\EN31380\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\core\\context.py:205\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m     )\n\u001b[1;32m--> 205\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    208\u001b[0m         master,\n\u001b[0;32m    209\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    220\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\EN31380\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\core\\context.py:444\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 444\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\EN31380\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\java_gateway.py:108\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 108\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    112\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    113\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    114\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "# Import necessary libraries for Spark and Delta Lake\n",
    "from delta import DeltaTable\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# Initialize Spark session\n",
    "# This is necessary to use Spark DataFrame and SQL functionalities\n",
    "def new_func():\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    return spark\n",
    "\n",
    "spark = new_func()\n",
    "# Set Spark configurations for Delta Lake and Parquet\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    concat,\n",
    "    concat_ws,\n",
    "    current_date,\n",
    "    date_format,\n",
    "    dayofmonth,\n",
    "    explode,\n",
    "    from_json,\n",
    "    hour,\n",
    "    lag,\n",
    "    lit,\n",
    "    month,\n",
    "    round,\n",
    "    struct,\n",
    "    to_csv,\n",
    "    to_json,\n",
    "    trim,\n",
    "    udf,\n",
    "    when,\n",
    "    year,\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType,\n",
    "    DateType,\n",
    "    DecimalType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "\n",
    "spark.conf.set('spark.sql.caseSensitive', True)\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cbb2b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tip_cast(tipo, precision=None, scale=None):\n",
    "    \"\"\"\n",
    "    Devuelve el tipo de dato de PySpark para castear columnas según el parámetro 'tipo'.\n",
    "    - tipo: 'decimal', 'integer', 'string', 'float', 'double', 'date', 'timestamp', etc.\n",
    "    - precision, scale: solo para decimal.\n",
    "    \"\"\"\n",
    "    from pyspark.sql.types import DecimalType, IntegerType, StringType, FloatType, DoubleType, DateType, TimestampType\n",
    "\n",
    "    if tipo == \"decimal\":\n",
    "        if precision is not None and scale is not None:\n",
    "            return DecimalType(precision=precision, scale=scale)\n",
    "        else:\n",
    "            return DecimalType()\n",
    "    elif tipo == \"integer\":\n",
    "        return IntegerType()\n",
    "    elif tipo == \"string\":\n",
    "        return StringType()\n",
    "    elif tipo == \"float\":\n",
    "        return FloatType()\n",
    "    elif tipo == \"double\":\n",
    "        return DoubleType()\n",
    "    elif tipo == \"date\":\n",
    "        return DateType()\n",
    "    elif tipo == \"timestamp\":\n",
    "        return TimestampType()\n",
    "    else:\n",
    "        raise ValueError(f\"Tipo '{tipo}' no soportado.\")\n",
    "\n",
    "## Ejemplo de uso:\n",
    "# tipo_decimal = get_tip_cast(\"decimal\", 10, 2)\n",
    "# tipo_entero = get_tip_cast(\"integer\")\n",
    "# tipo_texto = get_tip_cast(\"string\")\n",
    "# df = df.withColumn(\"columna_decimal\", F.col(\"columna_decimal\").cast(tipo_decimal))\n",
    "# df = df.withColumn(\"columna_entero\", F.col(\"columna_entero\").cast(tipo_entero))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d44412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(dforigen, selected_cols, new_col_names):\n",
    "    df_filtered = dforigen.select(selected_cols)\n",
    "    df_renamed = df_filtered.toDF(*new_col_names)\n",
    "    return df_renamed\n",
    "\n",
    "\n",
    "## Ejemplo de uso:\n",
    "## Supón que tienes un DataFrame con columnas: [\"A\", \"B\", \"C\"]\n",
    "#df_renamed = rename_columns(df, [\"A\", \"B\"], [\"Columna1\", \"Columna2\"])\n",
    "## Ahora df_renamed tendrá solo dos columnas: [\"Columna1\", \"Columna2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc084d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_write_params_overwriteschema(is_dev):\n",
    "    if is_dev:\n",
    "        write_params = True\n",
    "    else: \n",
    "        write_params = False\n",
    "    return write_params\n",
    "\n",
    "## No entiendo el uso de la funcion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c57a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_table_lh(workspace, lakehouse, schema, table):\n",
    "    # Validación simple de parámetros\n",
    "    if any(\" \" in str(x) for x in [workspace, lakehouse, schema, table]):\n",
    "        raise ValueError(\"Ningún parámetro debe contener espacios en blanco.\")\n",
    "    # Construcción de la ruta\n",
    "    table_path = f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/{lakehouse}.Lakehouse/Tables/{schema}/{table}\"\n",
    "    # Lectura de la tabla\n",
    "    df = spark.read.load(table_path)\n",
    "    return df\n",
    "\n",
    "## Ejemplo de uso:\n",
    "# Esta función se utiliza para leer una tabla de un Lakehouse en OneLake.\n",
    "## Supón que tienes los siguientes datos:\n",
    "# workspace = \"miworkspace\"\n",
    "# lakehouse = \"milakehouse\"\n",
    "# schema = \"miesquema\"\n",
    "# table = \"mitabla\"\n",
    "\n",
    "## Llamas a la función para leer la tabla:\n",
    "#df = read_table_lh(workspace, lakehouse, schema, table)\n",
    "\n",
    "## Ahora puedes trabajar con el DataFrame 'df', por ejemplo:\n",
    "#df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8a95d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_or_create_schema(workspace, lakehouse, schema_name):\n",
    "    list_of_schemas = []\n",
    "    for schema in notebookutils.fs.ls( f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/{lakehouse}.Lakehouse/Tables/\"):\n",
    "        list_of_schemas.append(schema.name)\n",
    "    if schema_name not in list_of_schemas:\n",
    "        notebookutils.fs.mkdirs(f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/{lakehouse}.Lakehouse/Tables/{schema_name}\")\n",
    "        print(f\"El esquema {schema_name} se ha creado.\")\n",
    "    else:\n",
    "        print(f\"El esquema {schema_name} ya existe en el entorno de trabajo especificado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282143e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario para traducir números de mes a nombres en español\n",
    "mes_nombre = {1: \"Enero\", 2: \"Febrero\", 3: \"Marzo\", 4: \"Abril\",\n",
    "              5: \"Mayo\", 6: \"Junio\", 7: \"Julio\", 8: \"Agosto\",\n",
    "              9: \"Septiembre\", 10: \"Octubre\", 11: \"Noviembre\", 12: \"Diciembre\"}\n",
    "\n",
    "@udf(StringType())\n",
    "def cambiar_idioma_mes(month):\n",
    "    return mes_nombre.get(month, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "096960ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_segmentos_fecha(columna):\n",
    "    \"\"\"\n",
    "    Extrae diferentes segmentos de una columna que representa fechas.\n",
    "\n",
    "    :param columna: La columna que contiene las fechas.\n",
    "    :return: Una lista con nuevas columnas que representan el año, mes y nombre del mes de la fecha.\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import year, month, dayofmonth, col\n",
    "\n",
    "    # Extrae el año\n",
    "    año = year(col(columna)).alias(f\"{columna}_Año\")\n",
    "\n",
    "    # Extrae el mes (numérico)\n",
    "    mes_numérico = month(col(columna)).alias(f\"{columna}_Mes\")\n",
    "\n",
    "    # Convierte el mes numérico a nombre en español\n",
    "    mes_nombre = cambiar_idioma_mes(mes_numérico).alias(f\"{columna}_MesNombre\")\n",
    "\n",
    "    # Extrae el día del mes\n",
    "    dia_del_mes = dayofmonth(col(columna)).alias(f\"{columna}_Dia\")\n",
    "\n",
    "    return [año, mes_nombre, dia_del_mes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bec7e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_segmentos_hora(columna):\n",
    "   return [hour(col(columna)).alias(f\"{columna}_Hora\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2869f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traducir_mapeo_diccionario(dictionary): \n",
    "    return udf(lambda col: dictionary.get(col),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a6e1b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_url(url, headers=None, params=None):\n",
    "    \"\"\"\n",
    "    Obtiene datos de una URL utilizando una solicitud GET a través de la biblioteca requests.\n",
    "    \n",
    "    :param url: URI de la API al que se realizará la solicitud.\n",
    "    :param headers: Diccionario de encabezados adicionales (opcional).\n",
    "    :param params: Diccionario de parámetros adicionales para la solicitud GET (opcional).\n",
    "    :return: Los datos decodificados del JSON si la respuesta es exitosa, None en caso contrario.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        \n",
    "        if response.ok:\n",
    "            # Añadir excepción más específica para errores de codificación JSON\n",
    "            return response.json()\n",
    "        else:\n",
    "            msg = f\"status {response.status_code}, details: {response.text}\"\n",
    "            raise Exception(f\"API call error: {msg}\")\n",
    "    \n",
    "    except requests.exceptions.JSONDecodeError as e:\n",
    "        print(f\"Decoding error: {e}\")\n",
    "        return None\n",
    "\n",
    "## Ejemplo de uso\n",
    "#import logging\n",
    "\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "#url = \"https://api.example.com/data\"\n",
    "#params = {\n",
    "#    \"param1\": \"value1\",\n",
    "#    \"param2\": \"value2\"\n",
    "#}\n",
    "#headers = {\n",
    "#    \"Authorization\": \"Bearer your_access_token_here\"\n",
    "#}\n",
    "\n",
    "#try:\n",
    "#    data = get_data_from_url(url, headers=headers, params=params)\n",
    "#    if data is not None:\n",
    "#        logging.info(\"Datos obtenidos exitosamente:\")\n",
    "#        logging.info(data)\n",
    "#    else:\n",
    "#        logging.error(\"Error al obtener los datos.\")\n",
    "#except Exception as e:\n",
    "#    logging.error(f\"Ocurrió un error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ec9dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_json_col(df, col_name, schema, list_selected_cols):\n",
    "    '''\n",
    "    df: DataFrame name we want to modify\n",
    "    col_name: Column name we want to normalize\n",
    "    shcema: JSON schema in PySpark types \n",
    "    selected_cols: Column names we want to select\n",
    "    '''\n",
    "    normalized_df = df.withColumn(col_name, from_json(col(col_name), schema))\n",
    "    normalized_df = normalized_df.withColumn(col_name, explode(col_name))\n",
    "    normalized_df = normalized_df.select([col(c) for c in list_selected_cols])\n",
    "    \n",
    "    return normalized_df\n",
    "\n",
    "## Ejemplo de uso:\n",
    "\n",
    "#from pyspark.sql import SparkSession\n",
    "\n",
    "## Inicializar una sesión de Spark\n",
    "#spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "\n",
    "## Ejemplo de DataFrame con columnas JSON\n",
    "#data = [\n",
    "#   (1, '{\"name\": \"Alice\", \"age\": 30, \"address\": {\"city\": \"New York\"}}'),\n",
    "#   (2, '{\"name\": \"Bob\", \"age\": 25, \"address\": {\"city\": \"Los Angeles\"}}')\n",
    "#]\n",
    "#columns = [\"id\", \"json_data\"]\n",
    "\n",
    "#df = spark.createDataFrame(data, columns)\n",
    "\n",
    "## Definir el esquema JSON\n",
    "#schema = {\n",
    "#    'id': IntegerType(),\n",
    "#    'name': StringType(),\n",
    "#    'age': IntegerType(),\n",
    "#    'address': StructType([\n",
    "#     ('city', StringType())\n",
    "#    ])\n",
    "#}\n",
    "\n",
    "# Columna a normalizar y las columnas seleccionadas\n",
    "#col_to_normalize = \"json_data\"\n",
    "#selected_columns = [\"id\", \"name\", \"city\"]\n",
    "\n",
    "# Llamar a la función normalize_json_col\n",
    "#normalized_df = normalize_json_col(df, col_to_normalize, schema, selected_columns)\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "#normalized_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c14fd01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_json_col(df, col_name, schema):\n",
    "    '''\n",
    "    df: DataFrame name we want to modify\n",
    "    col_name: Column name we want to normalize\n",
    "    shcema: JSON schema in PySpark types \n",
    "    '''\n",
    "    # Convert the JSON column into a structured format using from_json\n",
    "    normalized_df = df.withColumn(col_name, from_json(col(col_name), schema))\n",
    "    \n",
    "    return normalized_df\n",
    "\n",
    "## Ejemplo de uso:\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "\n",
    "#    data = [\n",
    "#        (1, '{\"name\": \"Alice\", \"age\": 30, \"address\": {\"city\": \"New York\"}}'),\n",
    "#        (2, '{\"name\": \"Bob\", \"age\": 25, \"address\": {\"city\": \"Los Angeles\"}}')\n",
    "#    ]\n",
    "#    columns = [\"id\", \"json_data\"]\n",
    "\n",
    "#    df = spark.createDataFrame(data, columns)\n",
    "\n",
    "    # Define the JSON schema\n",
    "#    schema = {\n",
    "#        'id': IntegerType(),\n",
    "#        'name': StringType(),\n",
    "#        'age': IntegerType(),\n",
    "#        'address': StructType([\n",
    "#            ('city', StringType())\n",
    "#        ])\n",
    "#    }\n",
    "\n",
    "#3    col_to_normalize = \"json_data\"\n",
    "\n",
    "## Normalize the column and select specific columns\n",
    "#    normalized_df = normalize_json_col(df, col_to_normalize, schema)\n",
    "\n",
    "## Use json_normalize to simplify the data extraction\n",
    "#    normalized_df_normalized = json_normalize(normalized_df.select(col_name))\n",
    "\n",
    "## Show the result\n",
    "# normalized_df_normalized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c1bc09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jerarquizar_fechas(df, lista_columnas):\n",
    "    # Verificar si todas las columnas están presentes en el DataFrame\n",
    "    if not all(col in df.columns for col in lista_columnas):\n",
    "        print(\"No todas las columnas estaban en el DataFrame, proceso no ejecutado.\")\n",
    "        return df\n",
    "\n",
    "    # Extraer segmentos de fecha (año, mes y día) para todas las columnas especificadas\n",
    "    fechas_jerarquizadas = []\n",
    "    for col in lista_columnas:\n",
    "        fecha_segmentos = extraer_segmentos_fecha(col)\n",
    "        if fecha_segmentos:\n",
    "            fechas_jerarquiaizadas.extend(fecha_segmentos)\n",
    "\n",
    "    # Seleccionar el DataFrame para incluir todas las columnas actuales y los nuevos segmentos de fecha\n",
    "    df = df.select(*df.columns, *fechas_jerarquizadas)\n",
    "\n",
    "    return df\n",
    "\n",
    "##Ejemplo de uso:\n",
    "#import pandas as pd\n",
    "\n",
    "## Creación del DataFrame\n",
    "#data = {\n",
    "#    'fecha1': ['2023-04-15', '2023-05-20', '2023-06-25'],\n",
    "#    'fecha2': ['2023-12-25', '2024-01-01', '2024-02-05']\n",
    "#}\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "## Lista de columnas a jerarquizar\n",
    "#columnas_a_jerarquizar = ['fecha1', 'fecha2']\n",
    "\n",
    "## Llamada a la función jerarquizar_fechas\n",
    "#resultado = jerarquizar_fechas(df, columnas_a_jerarquizar)\n",
    "\n",
    "#print(resultado)\n",
    "\n",
    "# fecha1   fecha2  año_f1  mes_f1  día_f1  año_f2  mes_f2  día_f2\n",
    "#0  2023-04-15  2023-12-25     2023      4       15        2023    12       25\n",
    "#1  2023-05-20  2024-01-01     2023      5       20        2024      1       01\n",
    "#2  2023-06-25  2024-02-05     2023      6       25        2024      2       05\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da652a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jerarquizar_horas(df, lista_columnas):\n",
    "    if all(col in df.columns for col in lista_columnas):\n",
    "        # Se extrae tiempo (hora, minutos y segundos)\n",
    "        columnas_jerarquia_fechas = []\n",
    " \n",
    "        for column in lista_columnas:\n",
    "            columnas_jerarquia_fechas.extend(extraer_segmentos_hora(column))\n",
    "        df = df.select(*df.columns, *columnas_jerarquia_fechas)\n",
    "    else:\n",
    "        print(\"No todas las columnas estaban en el DataFrame, proceso no ejecutado.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d40da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nvl(valor, reemplazo):\n",
    "    \"\"\"\n",
    "    Devuelve 'valor' si no es None, en caso contrario devuelve 'reemplazo'.\n",
    "    Equivalente a la función NVL de Oracle.\n",
    "    \"\"\"\n",
    "    return valor if valor is not None else reemplazo\n",
    "\n",
    "# Ejemplo de uso:\n",
    "#   print(nvl(None, 'sc'))  # Salida: sc\n",
    "#   print(nvl(5, 'sc'))     # Salida: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bc4321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pk(*args):\n",
    "    \"\"\"\n",
    "    Devuelve la concatenación de los argumentos recibidos, separados por '_'.\n",
    "    \"\"\"\n",
    "    return \"_\".join(str(arg) for arg in args)\n",
    "\n",
    "def nvl(valor, reemplazo):\n",
    "    \"\"\"\n",
    "    Devuelve 'valor' si no es None, en caso contrario devuelve 'reemplazo'.\n",
    "    Equivalente a la función NVL de Oracle.\n",
    "    \"\"\"\n",
    "    return valor if valor is not None else reemplazo\n",
    "\n",
    "# Ejemplo de uso:\n",
    "#   resultado = concatenar_campos(1, 20, \"ABC\")\n",
    "#   print(resultado)  # Salida: 1_20_ABC\n",
    "#   print(nvl(None, 'sc'))  # Salida: sc\n",
    "#   print(nvl(5, 'sc'))     # Salida: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2617bbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id valor\n",
      "6   4     g\n",
      "7   4     g\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_duplicates_pk(df, campos):\n",
    "    \"\"\"\n",
    "    Devuelve las filas del DataFrame que tienen valores duplicados en la combinación de los campos especificados.\n",
    "    \"\"\"\n",
    "    return df[df.duplicated(subset=campos, keep=False)]\n",
    "\n",
    "##Crear un DataFrame de ejemplo\n",
    "#df = pd.DataFrame({\n",
    "#    'id': [1, 2, 2, 3, 4, 4, 4,4],\n",
    "#    'valor': ['a', 'b', 'c', 'd', 'e', 'f', 'g','g']\n",
    "#})\n",
    "\n",
    "#duplicados = get_duplicates_pk(df, ['id', 'valor'])\n",
    "#print(duplicados)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
