{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fb924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === ParÃ¡metros principales del proceso ===\n",
    "tabla_origen = \"CONT_TADENDA\"\n",
    "tabla_destino = \"lk_adenda\"\n",
    "schema_config = \"config\"\n",
    "schema_origen= \"staging\"\n",
    "schema_destino = \"origin1\"\n",
    "lakehouse_origen = \"raw_lh_tte_origen1\"\n",
    "lakehouse_destino = \"std_lh_tte_dominioEnagas\"\n",
    "fecha_inicio = \"1900-01-01\"\n",
    "workspace=\"ws_tte_poc_data\"\n",
    "usuario=\"adm_tte\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2923548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit\n",
    "from functools import reduce\n",
    "import re\n",
    "from delta.tables import DeltaTable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9bdac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_mapping(df_origen, mapping_df):\n",
    "    # Crear un diccionario con los tipos de datos vÃ¡lidos en pyspark.sql.types\n",
    "    # Esto permite mapear un string como \"StringType\" a la clase correspondiente\n",
    "    tipos_validos = {\n",
    "        t.__name__: t for t in [\n",
    "            StringType, ShortType, IntegerType, LongType,\n",
    "            DoubleType, FloatType, BooleanType, DateType, TimestampType\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Recolectar todas las columnas origen definidas en el DataFrame de mapeo\n",
    "    columnas_origen = [row['columna_origen'] for row in mapping_df.collect()]\n",
    "\n",
    "    # Verificar si esas columnas realmente existen en el DataFrame de entrada\n",
    "    columnas_faltantes = [col for col in columnas_origen if col not in df_origen.columns]\n",
    "\n",
    "    # Si alguna columna no existe en df_origen, lanzar un error\n",
    "    if columnas_faltantes:\n",
    "        raise ValueError(f\"Las siguientes columnas no existen en el DataFrame origen: {', '.join(columnas_faltantes)}\")\n",
    "\n",
    "    # Lista para guardar las transformaciones que se aplicarÃ¡n (cast + rename)\n",
    "    columnas_transformadas = []\n",
    "\n",
    "    # Iterar sobre cada fila del DataFrame de mapeo\n",
    "    for row in mapping_df.collect():\n",
    "        col_origen = row['columna_origen']      # Nombre original de la columna\n",
    "        col_destino = row['columna_destino']    # Nombre deseado de la columna\n",
    "        tipo_dato_str = row['tipo_dato']        # Tipo de dato como string (ej. \"IntegerType\")\n",
    "\n",
    "        # Validar que el tipo de dato indicado sea vÃ¡lido\n",
    "        if tipo_dato_str not in tipos_validos:\n",
    "            raise ValueError(f\"Tipo de dato '{tipo_dato_str}' no es vÃ¡lido. Verifica el valor en la tabla de mapeo.\")\n",
    "\n",
    "        # Obtener la clase correspondiente al tipo de dato\n",
    "        tipo_dato = tipos_validos[tipo_dato_str]\n",
    "\n",
    "        # Agregar la transformaciÃ³n a la lista: casteo del tipo y renombre de la columna\n",
    "        columnas_transformadas.append(\n",
    "            F.col(col_origen).cast(tipo_dato()).alias(col_destino)\n",
    "        )\n",
    "\n",
    "    # Aplicar todas las transformaciones en un solo select y devolver el DataFrame resultante\n",
    "    df_transformado = df_origen.select(*columnas_transformadas)\n",
    "    return df_transformado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cea6f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_claves_pk(validation_df):\n",
    "    \"\"\"\n",
    "    Extrae las claves primarias Ãºnicas y limpias de un DataFrame de configuraciÃ³n.\n",
    "\n",
    "    Args:\n",
    "        validation_df (DataFrame): DataFrame que contiene la columna 'pk_columna',\n",
    "                                   donde se especifican las claves primarias como una lista separada por comas.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de claves primarias Ãºnicas y limpias.\n",
    "    \"\"\"\n",
    "    # Filtrar filas donde pk_columna es nula o vacÃ­a (solo espacios)\n",
    "    df_filtrado = validation_df.filter(\"pk_columna IS NOT NULL AND TRIM(pk_columna) != ''\")\n",
    "\n",
    "    # Separar claves por coma, eliminar espacios y descartar vacÃ­os\n",
    "    claves = df_filtrado.select(\"pk_columna\").distinct().rdd \\\n",
    "        .flatMap(lambda x: [col.strip() for col in x[0].split(\",\") if col and col.strip()])\n",
    "\n",
    "    # Devolver lista Ãºnica y ordenada\n",
    "    return sorted(set(claves.collect()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4039a618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_df_error(pk_cols, col, regla, mensaje_error, severidad, fecha_ejecucion):\n",
    "    \"\"\"\n",
    "    Crea un DataFrame de errores con un esquema explÃ­cito para una validaciÃ³n fallida.\n",
    "\n",
    "    Args:\n",
    "        pk_cols (list): Lista de columnas que representan la clave primaria.\n",
    "        col (str): Nombre de la columna donde se detectÃ³ el error.\n",
    "        regla (str): Tipo de regla de validaciÃ³n aplicada.\n",
    "        mensaje_error (str): Mensaje descriptivo del error encontrado.\n",
    "        severidad (str): Nivel de severidad asignado a la regla.\n",
    "        fecha_ejecucion (str): Timestamp de la ejecuciÃ³n actual.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame de Spark con un solo registro de error y esquema definido.\n",
    "    \"\"\"\n",
    "    schema_error = StructType(\n",
    "        [StructField(pk, StringType(), True) for pk in pk_cols] +\n",
    "        [\n",
    "            StructField(\"columna\", StringType(), True),\n",
    "            StructField(\"tipo_regla\", StringType(), True),\n",
    "            StructField(\"error\", StringType(), True),\n",
    "            StructField(\"severidad\", StringType(), True),\n",
    "            StructField(\"fecha_ejecucion\", StringType(), True),\n",
    "        ]\n",
    "    )\n",
    "    error_data = {pk: None for pk in pk_cols}\n",
    "    error_data.update({\n",
    "        \"columna\": col,\n",
    "        \"tipo_regla\": regla,\n",
    "        \"error\": mensaje_error,\n",
    "        \"severidad\": severidad,\n",
    "        \"fecha_ejecucion\": fecha_ejecucion\n",
    "    })\n",
    "    return spark.createDataFrame([error_data], schema=schema_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a95c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_validaciones(df, validation_config_df):\n",
    "    \"\"\"\n",
    "    Aplica un conjunto de reglas de validaciÃ³n sobre un DataFrame principal,\n",
    "    utilizando una configuraciÃ³n de reglas definidas en otro DataFrame.\n",
    "\n",
    "    Por cada regla violada, se crea un registro de error que es acumulado y,\n",
    "    al finalizar, todos los errores son escritos en la carpeta 'Files' del lakehouse correspondiente.\n",
    "\n",
    "    Reglas soportadas:\n",
    "        - not_null\n",
    "        - unique\n",
    "        - range\n",
    "        - regex_match\n",
    "        - value_in_list\n",
    "        - fk_exists (verifica existencia en tabla referencial Delta)\n",
    "        - date_range\n",
    "        - min_value\n",
    "        - max_value\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame principal a validar.\n",
    "        validation_config_df (DataFrame): DataFrame con las reglas de validaciÃ³n.\n",
    "            Se espera que tenga las siguientes columnas:\n",
    "                - regla_tipo\n",
    "                - columna_1\n",
    "                - columna_2 (opcional para algunas reglas)\n",
    "                - valor (usado en reglas como range, regex, value_in_list, etc.)\n",
    "                - tabla_referencia / columna_referencia (para fk_exists)\n",
    "                - descripcion (mensaje del error)\n",
    "                - severidad\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame de errores encontrados durante las validaciones.\n",
    "\n",
    "    Nota:\n",
    "        Esta funciÃ³n depende de variables globales que deben estar definidas antes de su ejecuciÃ³n:\n",
    "            - workspace: Nombre del Fabric workspace.\n",
    "            - lakehouse_destino: Lakehouse donde se encuentra la tabla de referencia (si aplica).\n",
    "            - schema_destino: Carpeta del esquema en el lakehouse.\n",
    "            - tabla_destino: Nombre de la tabla origen que se estÃ¡ validando.\n",
    "    \"\"\"\n",
    "    errores = []\n",
    "    fecha_ejecucion = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    pk_cols = obtener_claves_pk(validation_config_df)\n",
    "\n",
    "    for row in validation_config_df.toLocalIterator():\n",
    "        regla = row['regla_tipo']\n",
    "        col = row['columna_1']\n",
    "        descripcion = row['descripcion']\n",
    "        severidad = row['severidad']\n",
    "        operador = row['operador']\n",
    "        valor = row['valor']\n",
    "        col2 = row['columna_2']\n",
    "        tabla_ref = row['tabla_referencia']\n",
    "        col_ref = row['columna_referencia']\n",
    "\n",
    "        df_error = None\n",
    "\n",
    "        try:\n",
    "            if regla == \"not_null\":\n",
    "                df_error = df.filter(F.col(col).isNull())\n",
    "\n",
    "            elif regla == \"unique\":\n",
    "                df_error = (\n",
    "                    df.groupBy(col).count().filter(\"count > 1\")\n",
    "                    .join(df, on=col, how=\"inner\")\n",
    "                )\n",
    "\n",
    "            elif regla == \"range\":\n",
    "                rango = [float(x.strip()) for x in valor.split(\",\")]\n",
    "                if len(rango) != 2:\n",
    "                    raise ValueError(f\"Regla 'range' mal definida: se esperaban 2 valores, se recibiÃ³: '{valor}'\")\n",
    "                min_val, max_val = rango\n",
    "                df_error = df.filter(~((F.col(col) >= min_val) & (F.col(col) <= max_val)))\n",
    "\n",
    "            elif regla == \"regex_match\":\n",
    "                regex_udf = F.udf(lambda x: not re.match(valor, x) if x else True, StringType())\n",
    "                df_error = df.filter(regex_udf(F.col(col)))\n",
    "\n",
    "            elif regla == \"value_in_list\":\n",
    "                try:\n",
    "                    lista = [float(x.strip()) for x in valor.split(\",\") if x.strip()]\n",
    "                    df_error = df.filter(~F.col(col).cast(\"double\").isin(lista))\n",
    "                except:\n",
    "                    lista = [x.strip() for x in valor.split(\",\") if x.strip()]\n",
    "                    df_error = df.filter(~F.col(col).isin(lista))\n",
    "\n",
    "            elif regla == \"fk_exists\":\n",
    "                try:\n",
    "                    ref_df_path = f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/{lakehouse_destino}.Lakehouse/Tables/{schema_destino}/{tabla_ref}\"\n",
    "\n",
    "                    ref_df_temp = spark.read.format(\"delta\").load(ref_df_path)\n",
    "\n",
    "                    if col_ref not in ref_df_temp.columns:\n",
    "                        raise Exception(f\"La columna '{col_ref}' no existe en la tabla '{tabla_ref}'.\")\n",
    "\n",
    "                    ref_df = F.broadcast(ref_df_temp.select(col_ref).distinct())\n",
    "                    df_error = df.join(ref_df, df[col] == ref_df[col_ref], \"left_anti\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    mensaje_error = f\"No se pudo acceder a la tabla '{tabla_ref}' o columna '{col_ref}': {str(e)}\"\n",
    "                    df_error = crear_df_error(pk_cols, col, regla, mensaje_error, severidad, fecha_ejecucion)\n",
    "                    errores.append(df_error)\n",
    "                    continue\n",
    "\n",
    "            elif regla == \"date_range\":\n",
    "                df_error = df.filter(\n",
    "                    ~((F.col(col) >= F.to_timestamp(F.lit(valor))) & (F.col(col) <= F.to_timestamp(F.lit(col2))))\n",
    "                )\n",
    "\n",
    "            elif regla == \"min_value\":\n",
    "                df_error = df.filter(F.col(col) < F.lit(valor))\n",
    "\n",
    "            elif regla == \"max_value\":\n",
    "                df_error = df.filter(F.col(col) > F.lit(valor))\n",
    "\n",
    "        except Exception as e:\n",
    "            mensaje_error = f\"Error procesando regla '{regla}' para columna '{col}': {str(e)}\"\n",
    "            print(f\"âš ï¸ {mensaje_error}\")\n",
    "            df_error = crear_df_error(pk_cols, col, regla, mensaje_error, severidad, fecha_ejecucion)\n",
    "            errores.append(df_error)\n",
    "            continue\n",
    "\n",
    "        if df_error is not None and not df_error.rdd.isEmpty():\n",
    "            for pk in pk_cols:\n",
    "                if pk not in df_error.columns:\n",
    "                    df_error = df_error.withColumn(pk, F.lit(None))\n",
    "\n",
    "            df_error = df_error.withColumn(\"error\", F.lit(descripcion)) \\\n",
    "                               .withColumn(\"severidad\", F.lit(severidad)) \\\n",
    "                               .withColumn(\"columna\", F.lit(col)) \\\n",
    "                               .withColumn(\"tipo_regla\", F.lit(regla)) \\\n",
    "                               .withColumn(\"fecha_ejecucion\", F.lit(fecha_ejecucion)) \\\n",
    "                               .select(*pk_cols, \"columna\", \"tipo_regla\", \"error\", \"severidad\", \"fecha_ejecucion\")\n",
    "\n",
    "            errores.append(df_error)\n",
    "\n",
    "    if errores:\n",
    "        df_errores_final = reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), errores)\n",
    "    else:\n",
    "        df_errores_final = spark.createDataFrame([], schema=crear_df_error(pk_cols, \"\", \"\", \"\", \"\", \"\").schema)\n",
    "\n",
    "    ruta_salida = f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/{lakehouse_destino}.Lakehouse/Files/valitation_error/{schema_destino}/{tabla_destino}/val_errors_{tabla_destino}_{fecha_ejecucion}\"\n",
    "    df_errores_final.write.mode(\"overwrite\").option(\"header\", True).csv(ruta_salida)\n",
    "\n",
    "    return df_errores_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf5588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def registrar_auditoria_carga(ruta_auditoria, total_registros, registros_insertados,registros_actualizados):\n",
    "    \"\"\"\n",
    "    Registra informaciÃ³n de auditorÃ­a de un proceso de UPSERT en un archivo CSV en OneLake.\n",
    "\n",
    "    Args:\n",
    "        ruta_auditoria (str): Ruta del directorio en OneLake donde guardar el log de auditorÃ­a.\n",
    "        nombre_tabla (str): Nombre de la tabla de destino.\n",
    "        usuario (str): Usuario que ejecuta el proceso.\n",
    "        total_registros (int): NÃºmero total de registros procesados.\n",
    "        registros_insertados (int): Registros insertados en la tabla.\n",
    "        registros_actualizados (int): Registros actualizados en la tabla.\n",
    "        schema_destino (str): Esquema en el lakehouse de destino.\n",
    "        lakehouse_origen (str): Nombre del lakehouse de origen.\n",
    "        lakehouse_destino (str): Nombre del lakehouse de destino.\n",
    "        workspace (str): Nombre del workspace de Microsoft Fabric.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        raise ValueError(\"No hay sesiÃ³n activa de Spark.\")\n",
    "\n",
    "    fecha_carga = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    fecha_ejecucion = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    datos_auditoria = [{\n",
    "        \"tabla\": {tabla_destino},\n",
    "        \"usuario\": {usuario},\n",
    "        \"fecha_carga\": fecha_carga,\n",
    "        \"fecha_ejecucion\": fecha_ejecucion,\n",
    "        \"total_registros\": str(total_registros),\n",
    "        \"registros_insertados\": str(registros_insertados),\n",
    "        \"registros_actualizados\": str(registros_actualizados),\n",
    "        \"schema_destino\": {schema_destino},\n",
    "        \"lakehouse_origen\": {lakehouse_origen},\n",
    "        \"lakehouse_destino\": {lakehouse_destino},\n",
    "        \"workspace\": {workspace}\n",
    "    }]\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"tabla\", StringType(), True),\n",
    "        StructField(\"usuario\", StringType(), True),\n",
    "        StructField(\"fecha_carga\", StringType(), True),\n",
    "        StructField(\"fecha_ejecucion\", StringType(), True),\n",
    "        StructField(\"total_registros\", StringType(), True),\n",
    "        StructField(\"registros_insertados\", StringType(), True),\n",
    "        StructField(\"registros_actualizados\", StringType(), True),\n",
    "        StructField(\"schema_destino\", StringType(), True),\n",
    "        StructField(\"lakehouse_origen\", StringType(), True),\n",
    "        StructField(\"lakehouse_destino\", StringType(), True),\n",
    "        StructField(\"workspace\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    df_auditoria = spark.createDataFrame(datos_auditoria, schema=schema)\n",
    "\n",
    "    # Ruta con timestamp Ãºnico para evitar sobreescritura\n",
    "    ruta_completa = f\"{ruta_auditoria}/auditoria_{tabla_destino}_{fecha_carga.replace('-', '')}_{datetime.now().strftime('%H%M%S')}.csv\"\n",
    "\n",
    "    df_auditoria.write.mode(\"overwrite\").option(\"header\", True).csv(ruta_completa)\n",
    "\n",
    "    print(f\"ðŸ“ AuditorÃ­a registrada en: {ruta_completa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f8d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hacer_upsert(df_nuevos, ruta_tabla, claves_pk, auditoria_path=None):\n",
    "    \"\"\"\n",
    "    Realiza un upsert sobre una tabla Delta en una ruta ABFSS y agrega informaciÃ³n de auditorÃ­a.\n",
    "    TambiÃ©n aÃ±ade los campos 'fecha_carga' y 'usuario' al DataFrame destino.\n",
    "    \n",
    "    Args:\n",
    "        df_nuevos (DataFrame): DataFrame con los nuevos datos.\n",
    "        ruta_tabla (str): Ruta ABFSS de la tabla Delta.\n",
    "        claves_pk (list): Lista de claves primarias.\n",
    "        auditoria_path (str, optional): Ruta para guardar logs de auditorÃ­a (Delta o Files).\n",
    "        usuario (str, optional): Usuario que ejecuta la carga. Por defecto: 'desconocido'.\n",
    "    \"\"\"\n",
    "\n",
    "    if not claves_pk:\n",
    "        raise ValueError(\"Debe proporcionar al menos una clave primaria para hacer el upsert.\")\n",
    "\n",
    "    # Campos de auditorÃ­a\n",
    "    fecha_carga_str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    df_nuevos = df_nuevos.withColumn(\"fecha_carga\", lit(fecha_carga_str)) \\\n",
    "                         .withColumn(\"usuario\", lit(usuario))\n",
    "\n",
    "    # Verificar existencia de tabla\n",
    "    try:\n",
    "        delta_table = DeltaTable.forPath(spark, ruta_tabla)\n",
    "        tabla_existe = True\n",
    "        print(f\"â„¹ï¸ Tabla encontrada en ruta: {ruta_tabla}\")\n",
    "    except:\n",
    "        tabla_existe = False\n",
    "        print(f\"ðŸ›ˆ La tabla no existe. Se crearÃ¡ en: {ruta_tabla}\")\n",
    "\n",
    "    if not tabla_existe:\n",
    "        # Crear tabla con nuevas columnas\n",
    "        df_nuevos.write.format(\"delta\").mode(\"overwrite\").save(ruta_tabla)\n",
    "        print(f\"âœ… Tabla creada con campos de auditorÃ­a en: {ruta_tabla}\")\n",
    "        num_insertados = df_nuevos.count()\n",
    "        num_actualizados = 0\n",
    "\n",
    "    else:\n",
    "        # Leer tabla actual\n",
    "        df_destino = spark.read.format(\"delta\").load(ruta_tabla)\n",
    "\n",
    "        # Comparar para auditorÃ­a\n",
    "        join_cond = [df_destino[pk] == df_nuevos[pk] for pk in claves_pk]\n",
    "        df_updates = df_destino.join(df_nuevos, join_cond, \"inner\")\n",
    "        df_inserts = df_nuevos.join(df_destino, join_cond, \"left_anti\")\n",
    "        num_actualizados = df_updates.count()\n",
    "        num_insertados = df_inserts.count()\n",
    "\n",
    "        # Crear vista temporal\n",
    "        df_nuevos.createOrReplaceTempView(\"source\")\n",
    "\n",
    "        cond = \" AND \".join([f\"target.{pk} = source.{pk}\" for pk in claves_pk])\n",
    "        columnas = df_nuevos.columns\n",
    "        set_expr = \", \".join([f\"target.{col} = source.{col}\" for col in columnas])\n",
    "        insert_cols = \", \".join(columnas)\n",
    "        insert_vals = \", \".join([f\"source.{col}\" for col in columnas])\n",
    "\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO delta.`{ruta_tabla}` AS target\n",
    "            USING source\n",
    "            ON {cond}\n",
    "            WHEN MATCHED THEN UPDATE SET {set_expr}\n",
    "            WHEN NOT MATCHED THEN INSERT ({insert_cols}) VALUES ({insert_vals})\n",
    "        \"\"\"\n",
    "        spark.sql(query)\n",
    "        print(f\"âœ… Merge completado en: {ruta_tabla}\")\n",
    "\n",
    "        auditoria_path = f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/{lakehouse_destino}.Lakehouse/Files/auditoria_cargas/{schema_destino}/{tabla_destino}\"\n",
    "        registrar_auditoria_carga(\n",
    "    ruta_auditoria=auditoria_path,\n",
    "    total_registros=df_nuevos.count(),\n",
    "    registros_insertados=num_insertados,\n",
    "    registros_actualizados=num_actualizados\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699303ec",
   "metadata": {},
   "source": [
    "Esquema de la tabla de auditorÃ­a: config.validacion_errores\n",
    "Campo\tTipo\tDescripciÃ³n\n",
    "pk\tstring\tIdentificador de la fila (uno o mÃ¡s)\n",
    "columna\tstring\tColumna evaluada\n",
    "error\tstring\tMensaje de error o regla fallida\n",
    "severidad\tstring\tNivel (alta, media, baja)\n",
    "tabla_destino\tstring\tNombre de la tabla donde se aplicÃ³ la validaciÃ³n\n",
    "fecha_carga\ttimestamp\tFecha y hora en que se detectÃ³ el error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529ed416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer datos desde lakehouse origen\n",
    "df_raw = spark.read.table(f\"{lakehouse_origen}.{schema_destino}.{tabla_origen}\") \\\n",
    "    .filter(F.col(\"fecha_carga\") >= F.lit(fecha_inicio))\n",
    "\n",
    "# Aplicar mapping\n",
    "df_standard = aplicar_mapping(df_raw, mapping_df)\n",
    "\n",
    "# Obtener claves primarias\n",
    "claves_pk = obtener_claves_pk(validation_config_df)\n",
    "\n",
    "# Aplicar validaciones\n",
    "df_errores = aplicar_validaciones(df_standard, validation_config_df)\n",
    "\n",
    "# Excluir registros invÃ¡lidos\n",
    "if df_errores.count() > 0:\n",
    "    pks_errores = df_errores.select(claves_pk).distinct()\n",
    "    df_validos = df_standard.join(pks_errores, on=claves_pk, how=\"left_anti\")\n",
    "else:\n",
    "    df_validos = df_standard\n",
    "\n",
    "# Realizar UPSERT de datos vÃ¡lidos\n",
    "hacer_upsert(df_validos, f\"{lakehouse_destino}.{schema_destino}.{tabla_destino}\", claves_pk)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b1a247",
   "metadata": {},
   "source": [
    "5. EJEMPLO DE USO\n",
    "\n",
    "Imagina que queremos procesar:\n",
    "\n",
    "Origen: CONT_TADENDA desde raw_lakehouse\n",
    "\n",
    "Destino: CONT_TADENDA_STANDARD en standard_lakehouse\n",
    "\n",
    "Primary Key: IDN_ADENDA\n",
    "\n",
    "Solo registros desde 2025-01-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce660f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla_origen = \"CONT_TADENDA\"\n",
    "tabla_destino = \"CONT_TADENDA_STANDARD\"\n",
    "lakehouse_origen = \"raw_lakehouse\"\n",
    "lakehouse_destino = \"standard_lakehouse\"\n",
    "schema_config = \"config\"\n",
    "schema_destino = \"origin1\"\n",
    "fecha_inicio = \"2025-01-01\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca80c0d2",
   "metadata": {},
   "source": [
    "Y ejecutar el pipeline tal cual estÃ¡.\n",
    "âœ… RESUMEN FINAL\n",
    "MÃ³dulo\tFuncionalidad\n",
    "Mapping dinÃ¡mico\tCast automÃ¡tico y mapeo entre origen y destino\n",
    "Validaciones\tExtensibles (not_null, fk_exists, regex, range, etc.)\n",
    "Upsert\tInsert/update usando claves primarias\n",
    "Performance\tbroadcast, incrementalidad y vistas temporales\n",
    "ReutilizaciÃ³n\tParametrizable para cualquier tabla, esquema y lakehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a366be",
   "metadata": {},
   "source": [
    "Â¿DÃ³nde implementar acciones correctivas tras validaciÃ³n?\n",
    "\n",
    "Las acciones correctivas deben colocarse despuÃ©s de aplicar las validaciones, pero antes del upsert, y solo en los registros vÃ¡lidos, o en un bloque separado de transformaciÃ³n de errores.\n",
    "\n",
    "Hay 2 formas de manejarlo segÃºn tu estrategia:\n",
    "\n",
    "ðŸ”§ OpciÃ³n 1: CorrecciÃ³n antes de la validaciÃ³n\n",
    "\n",
    "Para campos como not_null, si deseas imputar un valor cuando falte (en vez de lanzar error), puedes hacerlo antes de validar.\n",
    "\n",
    "Ventaja: El campo ya no fallarÃ¡ la validaciÃ³n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88813795",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standard = df_standard.withColumn(\n",
    "    \"codigo_adenda\",\n",
    "    F.when(F.col(\"codigo_adenda\").isNull(), F.lit(\"SC\")).otherwise(F.col(\"codigo_adenda\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4b9239",
   "metadata": {},
   "source": [
    "pciÃ³n 2: CorrecciÃ³n despuÃ©s de la validaciÃ³n\n",
    "\n",
    "Si prefieres identificar el error primero, y luego corregir solo ciertos casos permitidos, debes hacerlo tras la validaciÃ³n, sobre los errores.\n",
    "\n",
    "ðŸ§© Â¿CÃ³mo implementar esto de forma modular?\n",
    "\n",
    "Te recomiendo crear una funciÃ³n llamada corregir_datos(df) que aplique estas reglas de forma centralizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2169580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corregir_datos(df: DataFrame) -> DataFrame:\n",
    "    return (\n",
    "        df.withColumn(\"codigo_adenda\", \n",
    "                      F.when(F.col(\"codigo_adenda\").isNull(), F.lit(\"SC\"))\n",
    "                       .otherwise(F.col(\"codigo_adenda\")))\n",
    "          .withColumn(\"estado\",\n",
    "                      F.when(~F.col(\"estado\").isin(\"activo\", \"inactivo\", \"baja\"), F.lit(\"SACO\"))\n",
    "                       .otherwise(F.col(\"estado\")))\n",
    "          .withColumn(\"monto\",\n",
    "                      F.when(F.col(\"monto\").isNull(), F.lit(0))\n",
    "                       .otherwise(F.col(\"monto\")))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5e0103",
   "metadata": {},
   "source": [
    "Puedes aplicar esta funciÃ³n directamente a df_standard antes de las validaciones, o despuÃ©s de haber excluido los errores que no se pueden corregir.\n",
    "# 1. Aplicar mapping\n",
    "df_standard = aplicar_mapping(df_raw, mapping_df)\n",
    "\n",
    "# 2. Corregir datos segÃºn reglas\n",
    "df_standard = corregir_datos(df_standard)\n",
    "\n",
    "# 3. Validar\n",
    "df_errores = aplicar_validaciones(df_standard, validation_config_df)\n",
    "\n",
    "# 4. Excluir invÃ¡lidos\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ca8d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def aplicar_validaciones(df, validation_config_df):\n",
    "    errores = []\n",
    "    for row in validation_config_df.collect():\n",
    "        regla = row['regla_tipo']\n",
    "        col = row['columna_1']\n",
    "        descripcion = row['descripcion']\n",
    "        severidad = row['severidad']\n",
    "        pk_cols = row['pk_columna'].split(\",\")\n",
    "        operador = row['operador']\n",
    "        valor = row['valor']\n",
    "        col2 = row['columna_2']\n",
    "        tabla_ref = row['tabla_referencia']\n",
    "        col_ref = row['columna_referencia']\n",
    "\n",
    "        df_error = None\n",
    "\n",
    "        if regla == \"not_null\":\n",
    "            df_error = df.filter(F.col(col).isNull())\n",
    "\n",
    "        elif regla == \"unique\":\n",
    "            df_error = df.groupBy(col).count().filter(\"count > 1\").join(df, on=col, how=\"inner\")\n",
    "\n",
    "        elif regla == \"range\":\n",
    "            df_error = df.filter(~((F.col(col) >= F.lit(valor)) & (F.col(col) <= F.col(col2))))\n",
    "\n",
    "        elif regla == \"regex_match\":\n",
    "            regex_udf = F.udf(lambda x: not re.match(valor, x) if x else True, StringType())\n",
    "            df_error = df.filter(regex_udf(F.col(col)))\n",
    "\n",
    "        elif regla == \"value_in_list\":\n",
    "            lista = valor.split(\",\")\n",
    "            df_error = df.filter(~F.col(col).isin(lista))\n",
    "\n",
    "        elif regla == \"fk_exists\":\n",
    "            ref_df = F.broadcast(spark.read.format(\"delta\").load(f\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/{lakehouse_origen}.Lakehouse/Tables/{schema_origen}//{tabla_ref}\").select(col_ref).distinct())\n",
    "            df_error = df.join(ref_df, df[col] == ref_df[col_ref], \"left_anti\")\n",
    "\n",
    "        if df_error is not None:\n",
    "            for pk in pk_cols:\n",
    "                if pk not in df_error.columns:\n",
    "                    df_error = df_error.withColumn(pk, F.lit(None))\n",
    "\n",
    "            df_error = df_error.withColumn(\"error\", F.lit(descripcion)) \\\n",
    "                               .withColumn(\"severidad\", F.lit(severidad)) \\\n",
    "                               .withColumn(\"columna\", F.lit(col)) \\\n",
    "                               .select(*pk_cols, \"columna\", \"error\", \"severidad\")\n",
    "            errores.append(df_error)\n",
    "\n",
    "    # Combinar los DataFrames de errores usando reduce\n",
    "    return reduce(lambda df1, df2: df1.unionByName(df2), errores) if errores else spark.createDataFrame([], schema=\"pk string, columna string, error string, severidad string\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
